{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational AI with Structured Pydantic Extraction\n",
    "\n",
    "Testing three approaches to having a natural conversation and extracting structured Pydantic fields at the end — similar to how Rasa handles form-filling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai pydantic instructor dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# Set your API key (or set OPENAI_API_KEY env var before starting jupyter)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "# import from .env\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Pydantic Model\n",
    "\n",
    "This is the structured data we want to extract after a natural conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerRequest(BaseModel):\n",
    "    \"\"\"Structured data extracted from a customer conversation.\"\"\"\n",
    "    customer_name: str = Field(description=\"Customer's full name\")\n",
    "    email: str = Field(description=\"Customer's email address\")\n",
    "    issue_category: str = Field(description=\"Category: billing, technical, account, or other\")\n",
    "    issue_description: str = Field(description=\"Summary of the customer's issue\")\n",
    "    urgency: str = Field(description=\"low, medium, or high\")\n",
    "    resolution_requested: Optional[str] = Field(default=None, description=\"What the customer wants done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approach 1: OpenAI Structured Outputs (Native)\n",
    "\n",
    "Have a multi-turn conversation, then parse the final extraction into a Pydantic model using `response_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a friendly customer support agent.\n",
    "Have a natural conversation with the customer to understand their issue.\n",
    "Ask follow-up questions naturally until you have:\n",
    "- Their name\n",
    "- Their email\n",
    "- What category the issue falls into (billing, technical, account, or other)\n",
    "- A clear description of the issue\n",
    "- How urgent it is\n",
    "- What resolution they'd like\n",
    "\n",
    "Be conversational and empathetic. Don't ask for all info at once — \n",
    "gather it naturally over the conversation.\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_message: str) -> str:\n",
    "    \"\"\"Send a message and get a response. Run this cell multiple times to have a conversation.\"\"\"\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry you’re running into trouble — I’m happy to help. Could I get your name first, and a brief description of what’s happening with your account (for example: can’t sign in, locked out, incorrect billing, missing access, etc.)?\n"
     ]
    }
   ],
   "source": [
    "# Start the conversation — edit and re-run this cell to chat back and forth\n",
    "print(chat(\"Hi, I'm having a problem with my account\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks, John — I’m sorry that happened and I’ll get this sorted for you.\n",
      "\n",
      "I see the issue is billing-related and you said it’s pretty urgent. Before I start an investigation, a couple quick questions so I can escalate this right away:\n",
      "\n",
      "- Can you tell me the date of the charge(s) and the exact amounts shown on your card/bank statement?  \n",
      "- Do you have any invoice or transaction IDs (from your account billing history or your bank) or a screenshot you can attach?  \n",
      "- What payment method was used (card type) and can you share the last 4 digits of the card? (This helps me match the transactions — don’t send the full card number.)  \n",
      "- Is the duplicate charge showing on your bank/credit card statement as two separate postings, or is one a pending/authorization?  \n",
      "- What outcome would you prefer: an immediate refund to your card, an account credit, or an explanation and confirmation that the extra charge will be reversed?  \n",
      "- Any other details I should know (e.g., invoice number, subscription plan, whether you canceled/restarted a plan recently)?\n",
      "\n",
      "If you’d like me to begin right away, I can open a ticket and escalate to billing once you confirm the above. Typical refund reversals can take a few business days to appear depending on the bank; I’ll keep you updated at john@example.com as we progress. Do you want me to proceed now?\n"
     ]
    }
   ],
   "source": [
    "# Continue chatting (edit the message each time)\n",
    "print(chat(\"My name is John Smith, email john@example.com. I got double-charged on my last bill and it's pretty urgent.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks, John — I’m really sorry this happened and I’ll get it taken care of right away.\n",
      "\n",
      "I can start a refund for the duplicate charge to your original payment method. A couple quick details I need to locate the exact transactions and process the refund:\n",
      "\n",
      "- Date(s) of the charge(s) and the exact amounts shown on your statement  \n",
      "- Last 4 digits of the card used (please don’t send the full card number)  \n",
      "- Are both charges posted on your bank/credit-card statement, or is one still pending/authorization?  \n",
      "- Any invoice or transaction ID from your billing history (or a screenshot) if you have it\n",
      "\n",
      "If you don’t have all of that handy, that’s okay — I can search your account (John Smith / john@example.com) with the date and last 4 digits. Please confirm you want the refund returned to the original card (is that correct?), and that I should proceed now. I’ll email updates to john@example.com.\n",
      "\n",
      "Timeline: I’ll escalate this to our billing team immediately. Once we issue the refund it typically appears on the card in 3–10 business days depending on the bank; I’ll send a confirmation email as soon as the refund is processed and share a ticket number you can reference.\n",
      "\n",
      "Would you like me to proceed now using the details you’ve already provided, or do you want to share the transaction date/amounts and last 4 digits first?\n"
     ]
    }
   ],
   "source": [
    "print(chat(\"I'd like a refund for the duplicate charge please.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Pydantic object:\n",
      "customer_name='John Smith' email='john@example.com' issue_category='billing' issue_description='Double-charged on the last bill.' urgency='high' resolution_requested='Refund for the duplicate charge.'\n",
      "\n",
      "As dict:\n",
      "{'customer_name': 'John Smith', 'email': 'john@example.com', 'issue_category': 'billing', 'issue_description': 'Double-charged on the last bill.', 'urgency': 'high', 'resolution_requested': 'Refund for the duplicate charge.'}\n"
     ]
    }
   ],
   "source": [
    "# When ready, extract structured data from the conversation\n",
    "extraction_messages = messages + [\n",
    "    {\"role\": \"user\", \"content\": \"Please extract the structured customer request from this conversation.\"}\n",
    "]\n",
    "\n",
    "response = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=extraction_messages,\n",
    "    response_format=CustomerRequest,\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.parsed\n",
    "print(\"Extracted Pydantic object:\")\n",
    "print(result)\n",
    "print()\n",
    "print(\"As dict:\")\n",
    "print(result.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approach 2: Instructor Library\n",
    "\n",
    "Uses function calling under the hood with automatic retries and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "instructor_client = instructor.from_openai(OpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are a friendly customer support agent.\\nHave a natural conversation with the customer to understand their issue.\\nAsk follow-up questions naturally until you have:\\n- Their name\\n- Their email\\n- What category the issue falls into (billing, technical, account, or other)\\n- A clear description of the issue\\n- How urgent it is\\n- What resolution they'd like\\n\\nBe conversational and empathetic. Don't ask for all info at once — \\ngather it naturally over the conversation.\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructor extraction:\n",
      "customer_name='' email='' issue_category='other' issue_description='' urgency='medium' resolution_requested=None\n",
      "\n",
      "As dict:\n",
      "{'customer_name': '', 'email': '', 'issue_category': 'other', 'issue_description': '', 'urgency': 'medium', 'resolution_requested': None}\n"
     ]
    }
   ],
   "source": [
    "# Extract from the same conversation history we built above\n",
    "result_instructor = instructor_client.chat.completions.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    response_model=CustomerRequest,\n",
    "    messages=messages,\n",
    "    max_retries=2,  # auto-retries if validation fails\n",
    ")\n",
    "\n",
    "print(\"Instructor extraction:\")\n",
    "print(result_instructor)\n",
    "print()\n",
    "print(\"As dict:\")\n",
    "print(result_instructor.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approach 3: Rasa-Style Conversational Loop\n",
    "\n",
    "The AI chats naturally, decides when it has enough info, and signals completion. This is the closest to Rasa's form-filling behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "LOOP_SYSTEM_PROMPT = \"\"\"You are a friendly customer support agent.\n",
    "Have a natural conversation to collect the following information:\n",
    "- Customer's full name\n",
    "- Email address\n",
    "- Issue category (billing, technical, account, or other)\n",
    "- Description of their issue\n",
    "- Urgency (low, medium, high)\n",
    "- What resolution they want\n",
    "\n",
    "RULES:\n",
    "1. Be conversational and empathetic. Don't interrogate — chat naturally.\n",
    "2. Ask for missing info with follow-up questions, one or two at a time.\n",
    "3. When you have ALL required info, respond with EXACTLY this format:\n",
    "\n",
    "[COMPLETE]\n",
    "{\"customer_name\": \"...\", \"email\": \"...\", \"issue_category\": \"...\", \"issue_description\": \"...\", \"urgency\": \"...\", \"resolution_requested\": \"...\"}\n",
    "\n",
    "4. Do NOT output [COMPLETE] until you have every field.\n",
    "5. Before outputting [COMPLETE], confirm the details with the customer.\"\"\"\n",
    "\n",
    "\n",
    "def run_rasa_style_loop():\n",
    "    \"\"\"Interactive loop — type your messages, the AI chats back until it extracts all fields.\"\"\"\n",
    "    loop_messages = [{\"role\": \"system\", \"content\": LOOP_SYSTEM_PROMPT}]\n",
    "\n",
    "    # Get initial greeting\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=loop_messages + [{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "    )\n",
    "    greeting = response.choices[0].message.content\n",
    "    loop_messages.append({\"role\": \"user\", \"content\": \"Hi\"})\n",
    "    loop_messages.append({\"role\": \"assistant\", \"content\": greeting})\n",
    "    print(f\"Agent: {greeting}\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in (\"quit\", \"exit\"):\n",
    "            print(\"Conversation ended by user.\")\n",
    "            return None\n",
    "\n",
    "        loop_messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=loop_messages,\n",
    "        )\n",
    "        reply = response.choices[0].message.content\n",
    "        loop_messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "        if \"[COMPLETE]\" in reply:\n",
    "            # Extract JSON after the marker\n",
    "            json_str = reply.split(\"[COMPLETE]\")[1].strip()\n",
    "            # Show the confirmation text before [COMPLETE] if any\n",
    "            before = reply.split(\"[COMPLETE]\")[0].strip()\n",
    "            if before:\n",
    "                print(f\"Agent: {before}\\n\")\n",
    "\n",
    "            data = json.loads(json_str)\n",
    "            result = CustomerRequest(**data)\n",
    "            print(\"--- Extraction Complete ---\")\n",
    "            print(result.model_dump_json(indent=2))\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"Agent: {reply}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Hello there! How can I assist you today?\n",
      "\n",
      "Agent: It seems like your message didn't come through. How can I help you today?\n",
      "\n",
      "Agent: Oh no, I'm sorry to hear that! Could you tell me a little more about the problem you're experiencing?\n",
      "\n",
      "Agent: Hi again! What's going on? How can I assist you with your problem today?\n",
      "\n",
      "Agent: I see, you'd like a refund. Let's get that sorted out for you. Could you please tell me your full name and email address so I can locate your account?\n",
      "\n",
      "Agent: It seems like your last message didn't come through. Could you share your full name and email address so I can help with your refund?\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the interactive loop (type 'quit' to exit early)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m extracted = \u001b[43mrun_rasa_style_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mrun_rasa_style_loop\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAgent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgreeting\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_input.lower() \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     41\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConversation ended by user.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_triage/.venv/lib/python3.14/site-packages/ipykernel/kernelbase.py:1403\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1401\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1402\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1403\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_shell_context_var\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_shell_parent_ident\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/new_triage/.venv/lib/python3.14/site-packages/ipykernel/kernelbase.py:1448\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1445\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1446\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1447\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Run the interactive loop (type 'quit' to exit early)\n",
    "extracted = run_rasa_style_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extracted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Inspect the result\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mextracted\u001b[49m:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mName:       \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted.customer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmail:      \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted.email\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'extracted' is not defined"
     ]
    }
   ],
   "source": [
    "# Inspect the result\n",
    "if extracted:\n",
    "    print(f\"Name:       {extracted.customer_name}\")\n",
    "    print(f\"Email:      {extracted.email}\")\n",
    "    print(f\"Category:   {extracted.issue_category}\")\n",
    "    print(f\"Issue:      {extracted.issue_description}\")\n",
    "    print(f\"Urgency:    {extracted.urgency}\")\n",
    "    print(f\"Resolution: {extracted.resolution_requested}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approach Comparison\n",
    "\n",
    "| Approach | Pros | Cons |\n",
    "|---|---|---|\n",
    "| **Structured Outputs** | Native OpenAI, guaranteed schema | Extraction is a separate call at the end |\n",
    "| **Instructor** | Auto-retries, validation, clean API | Extra dependency |\n",
    "| **Rasa-style Loop** | AI decides when info is complete, most natural | Needs prompt engineering, JSON parsing can fail |\n",
    "\n",
    "For production, consider combining: use the **Rasa-style loop** for the conversation, then use **Instructor** or **Structured Outputs** for the final extraction (more reliable than parsing `[COMPLETE]` markers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Hybrid Approach (Recommended for Production)\n",
    "\n",
    "Chat naturally, then use structured outputs for reliable extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYBRID_SYSTEM = \"\"\"You are a friendly customer support agent.\n",
    "Have a natural conversation to understand the customer's issue.\n",
    "Collect: name, email, issue category, description, urgency, desired resolution.\n",
    "Be conversational. Don't interrogate.\n",
    "When you believe you have all info, say: \"I think I have everything I need. Let me summarize...\"\n",
    "and provide a natural-language summary for the customer to confirm.\"\"\"\n",
    "\n",
    "\n",
    "def run_hybrid_loop():\n",
    "    hybrid_messages = [{\"role\": \"system\", \"content\": HYBRID_SYSTEM}]\n",
    "\n",
    "    # Initial greeting\n",
    "    hybrid_messages.append({\"role\": \"user\", \"content\": \"Hello\"})\n",
    "    response = client.chat.completions.create(model=\"gpt-4o\", messages=hybrid_messages)\n",
    "    reply = response.choices[0].message.content\n",
    "    hybrid_messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    print(f\"Agent: {reply}\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in (\"quit\", \"exit\"):\n",
    "            return None\n",
    "\n",
    "        hybrid_messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        response = client.chat.completions.create(model=\"gpt-4o\", messages=hybrid_messages)\n",
    "        reply = response.choices[0].message.content\n",
    "        hybrid_messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        print(f\"Agent: {reply}\\n\")\n",
    "\n",
    "        # Check if agent thinks it has everything\n",
    "        if \"everything i need\" in reply.lower() or \"let me summarize\" in reply.lower():\n",
    "            confirm = input(\"You (confirm or correct): \")\n",
    "            hybrid_messages.append({\"role\": \"user\", \"content\": confirm})\n",
    "\n",
    "            if any(w in confirm.lower() for w in [\"yes\", \"correct\", \"looks good\", \"confirmed\", \"that's right\"]):\n",
    "                # Use structured outputs for reliable extraction\n",
    "                print(\"\\nExtracting structured data...\\n\")\n",
    "                extraction = client.beta.chat.completions.parse(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=hybrid_messages,\n",
    "                    response_format=CustomerRequest,\n",
    "                )\n",
    "                result = extraction.choices[0].message.parsed\n",
    "                print(\"--- Extraction Complete ---\")\n",
    "                print(result.model_dump_json(indent=2))\n",
    "                return result\n",
    "            else:\n",
    "                # Continue conversation to collect corrections\n",
    "                response = client.chat.completions.create(model=\"gpt-4o\", messages=hybrid_messages)\n",
    "                reply = response.choices[0].message.content\n",
    "                hybrid_messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "                print(f\"Agent: {reply}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Hi there! How can I help you today?\n",
      "\n",
      "Agent: Hey! What's going on? How can I assist you today?\n",
      "\n",
      "Agent: I'm sorry to hear that. Could you tell me a bit more about the problem you're experiencing? I'm here to help!\n",
      "\n",
      "Agent: Got it. Refunds are important, and I'd be happy to help with that. Could you tell me a little more about the situation? Like what the product or service was, and if there are any specific issues you encountered?\n",
      "\n",
      "Agent: Oh no, that sounds frustrating. I can definitely see why you'd want to get that fixed quickly. Could you let me know your name and email so I can look into your account and get this resolved for you?\n",
      "\n",
      "Agent: Thanks, Wael. Just to clarify, the email is \"wamansou@asu.edu,\" right? And to make sure we're on the same page, you're looking for a refund because you were double-charged on your last bill. Since you mentioned it's pretty urgent, we're aiming to resolve this as quickly as possible. Is there anything else you'd like to add or a specific timeline you're hoping for?\n",
      "\n",
      "Agent: I think I have everything I need. Let me summarize to make sure I've got it right:\n",
      "\n",
      "Your name is Wael, and your email is wamansou@asu.edu. You're seeking a refund because you've been double-charged on your last bill, and it's quite urgent for you to get this resolved promptly.\n",
      "\n",
      "Does that sound correct?\n",
      "\n",
      "\n",
      "Extracting structured data...\n",
      "\n",
      "--- Extraction Complete ---\n",
      "{\n",
      "  \"customer_name\": \"Wael\",\n",
      "  \"email\": \"wamansou@asu.edu\",\n",
      "  \"issue_category\": \"billing\",\n",
      "  \"issue_description\": \"Double-charged on the last bill.\",\n",
      "  \"urgency\": \"high\",\n",
      "  \"resolution_requested\": \"Refund for the double charge.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Run the hybrid loop\n",
    "hybrid_result = run_hybrid_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CustomerRequest'>\n",
      "{'customer_name': 'Wael', 'email': 'wamansou@asu.edu', 'issue_category': 'billing', 'issue_description': 'Double-charged on the last bill.', 'urgency': 'high', 'resolution_requested': 'Refund for the double charge.'}\n"
     ]
    }
   ],
   "source": [
    "if hybrid_result:\n",
    "    print(type(hybrid_result))  # <class 'CustomerRequest'>\n",
    "    print(hybrid_result.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approach 5: OpenAI Responses API (Newer than Chat Completions)\n",
    "\n",
    "The Responses API (`client.responses.parse()`) is OpenAI's newer extraction API. It uses `text_format=` instead of `response_format=` and returns `response.output_parsed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses API extraction:\n",
      "{\n",
      "  \"customer_name\": \"Jane Doe\",\n",
      "  \"email\": \"jane@company.com\",\n",
      "  \"issue_category\": \"technical\",\n",
      "  \"issue_description\": \"Software crashes when exporting reports after updating to version 3.2.1; started yesterday and is blocking the team.\",\n",
      "  \"urgency\": \"high\",\n",
      "  \"resolution_requested\": \"Roll back the update or patch the export.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Responses API: Extract from a conversation ---\n",
    "\n",
    "# Build a conversation first (reusing our messages from earlier, or start fresh)\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a customer support agent. Extract structured info from conversations.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I'm Jane Doe, jane@company.com. My software keeps crashing when I try to export reports. It's blocking my whole team.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm sorry to hear that, Jane. That sounds really frustrating. Can you tell me which version you're on and when this started happening?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Version 3.2.1, started yesterday after the update. We need this fixed ASAP — it's high priority. Ideally roll back the update or patch the export.\"},\n",
    "]\n",
    "\n",
    "# Use the new Responses API\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=conversation,\n",
    "    text_format=CustomerRequest,\n",
    ")\n",
    "\n",
    "result_responses_api = response.output_parsed\n",
    "print(\"Responses API extraction:\")\n",
    "print(result_responses_api.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Approach 6: OpenAI Agents SDK — Multi-Agent Triage (Most Rasa-Like)\n",
    "\n",
    "This is the **most advanced** and **closest to Rasa**. The OpenAI Agents SDK provides:\n",
    "\n",
    "- **Triage Agent** → routes customers to specialists (like Rasa's intent routing / stories)\n",
    "- **Specialist Agents** with handoffs (like Rasa's forms)\n",
    "- **Structured `output_type`** → Pydantic models as agent output (like Rasa's slots)\n",
    "- **SQLiteSession** → automatic multi-turn conversation memory (like Rasa's tracker store)\n",
    "- **Guardrails** → input validation before processing\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Customer → Triage Agent → Billing Agent (output_type=BillingTicket)\n",
    "                        → Technical Agent (output_type=TechnicalTicket)\n",
    "                        → Account Agent (output_type=AccountTicket)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-agents\n",
      "  Downloading openai_agents-0.10.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting griffe<2,>=1.5.6 (from openai-agents)\n",
      "  Downloading griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting mcp<2,>=1.19.0 (from openai-agents)\n",
      "  Downloading mcp-1.26.0-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: openai<3,>=2.19.0 in ./.venv/lib/python3.14/site-packages (from openai-agents) (2.24.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.12.3 in ./.venv/lib/python3.14/site-packages (from openai-agents) (2.12.5)\n",
      "Requirement already satisfied: requests<3,>=2.0 in ./.venv/lib/python3.14/site-packages (from openai-agents) (2.32.5)\n",
      "Collecting types-requests<3,>=2.0 (from openai-agents)\n",
      "  Downloading types_requests-2.32.4.20260107-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.12.2 in ./.venv/lib/python3.14/site-packages (from openai-agents) (4.15.0)\n",
      "Collecting colorama>=0.4 (from griffe<2,>=1.5.6->openai-agents)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio>=4.5 in ./.venv/lib/python3.14/site-packages (from mcp<2,>=1.19.0->openai-agents) (4.12.1)\n",
      "Collecting httpx-sse>=0.4 (from mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: httpx>=0.27.1 in ./.venv/lib/python3.14/site-packages (from mcp<2,>=1.19.0->openai-agents) (0.28.1)\n",
      "Collecting jsonschema>=4.20.0 (from mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading jsonschema-4.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading pydantic_settings-2.13.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pyjwt>=2.10.1 (from pyjwt[crypto]>=2.10.1->mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading pyjwt-2.11.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading python_multipart-0.0.22-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading sse_starlette-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting starlette>=0.27 (from mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading starlette-0.52.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: typing-inspection>=0.4.1 in ./.venv/lib/python3.14/site-packages (from mcp<2,>=1.19.0->openai-agents) (0.4.2)\n",
      "Collecting uvicorn>=0.31.1 (from mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading uvicorn-0.41.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.14/site-packages (from openai<3,>=2.19.0->openai-agents) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./.venv/lib/python3.14/site-packages (from openai<3,>=2.19.0->openai-agents) (0.11.1)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.14/site-packages (from openai<3,>=2.19.0->openai-agents) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.14/site-packages (from openai<3,>=2.19.0->openai-agents) (4.67.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.14/site-packages (from anyio>=4.5->mcp<2,>=1.19.0->openai-agents) (3.11)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.14/site-packages (from httpx>=0.27.1->mcp<2,>=1.19.0->openai-agents) (2026.2.25)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.14/site-packages (from httpx>=0.27.1->mcp<2,>=1.19.0->openai-agents) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.14/site-packages (from httpcore==1.*->httpx>=0.27.1->mcp<2,>=1.19.0->openai-agents) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.14/site-packages (from pydantic<3,>=2.12.3->openai-agents) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.14/site-packages (from pydantic<3,>=2.12.3->openai-agents) (2.41.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.14/site-packages (from requests<3,>=2.0->openai-agents) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.14/site-packages (from requests<3,>=2.0->openai-agents) (2.6.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.14/site-packages (from jsonschema>=4.20.0->mcp<2,>=1.19.0->openai-agents) (25.4.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.20.0->mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.25.0 (from jsonschema>=4.20.0->mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading rpds_py-0.30.0-cp314-cp314-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.14/site-packages (from pydantic-settings>=2.5.2->mcp<2,>=1.19.0->openai-agents) (1.2.1)\n",
      "Collecting cryptography>=3.4.0 (from pyjwt[crypto]>=2.10.1->mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading cryptography-46.0.5-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=2.0.0 (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading cffi-2.0.0-cp314-cp314-macosx_11_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp<2,>=1.19.0->openai-agents)\n",
      "  Downloading pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: click>=7.0 in ./.venv/lib/python3.14/site-packages (from uvicorn>=0.31.1->mcp<2,>=1.19.0->openai-agents) (8.3.1)\n",
      "Downloading openai_agents-0.10.2-py3-none-any.whl (404 kB)\n",
      "Downloading griffe-1.15.0-py3-none-any.whl (150 kB)\n",
      "Downloading mcp-1.26.0-py3-none-any.whl (233 kB)\n",
      "Downloading types_requests-2.32.4.20260107-py3-none-any.whl (20 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading jsonschema-4.26.0-py3-none-any.whl (90 kB)\n",
      "Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Downloading pydantic_settings-2.13.1-py3-none-any.whl (58 kB)\n",
      "Downloading pyjwt-2.11.0-py3-none-any.whl (28 kB)\n",
      "Downloading cryptography-46.0.5-cp311-abi3-macosx_10_9_universal2.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-2.0.0-cp314-cp314-macosx_11_0_arm64.whl (181 kB)\n",
      "Downloading python_multipart-0.0.22-py3-none-any.whl (24 kB)\n",
      "Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.30.0-cp314-cp314-macosx_11_0_arm64.whl (353 kB)\n",
      "Downloading sse_starlette-3.2.0-py3-none-any.whl (12 kB)\n",
      "Downloading starlette-0.52.1-py3-none-any.whl (74 kB)\n",
      "Downloading uvicorn-0.41.0-py3-none-any.whl (68 kB)\n",
      "Downloading pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Installing collected packages: uvicorn, types-requests, rpds-py, python-multipart, pyjwt, pycparser, httpx-sse, colorama, starlette, referencing, griffe, cffi, sse-starlette, pydantic-settings, jsonschema-specifications, cryptography, jsonschema, mcp, openai-agents\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19/19\u001b[0m [openai-agents]━━\u001b[0m \u001b[32m18/19\u001b[0m [openai-agents]\n",
      "\u001b[1A\u001b[2KSuccessfully installed cffi-2.0.0 colorama-0.4.6 cryptography-46.0.5 griffe-1.15.0 httpx-sse-0.4.3 jsonschema-4.26.0 jsonschema-specifications-2025.9.1 mcp-1.26.0 openai-agents-0.10.2 pycparser-3.0 pydantic-settings-2.13.1 pyjwt-2.11.0 python-multipart-0.0.22 referencing-0.37.0 rpds-py-0.30.0 sse-starlette-3.2.0 starlette-0.52.1 types-requests-2.32.4.20260107 uvicorn-0.41.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models defined.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from agents import Agent, Runner, handoff, RunContextWrapper, InputGuardrail, GuardrailFunctionOutput\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────\n",
    "# 1. Define structured output models (like Rasa slots)\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "class BillingTicket(BaseModel):\n",
    "    \"\"\"Structured output for billing issues.\"\"\"\n",
    "    customer_name: str = Field(description=\"Customer's full name\")\n",
    "    email: str = Field(description=\"Customer's email\")\n",
    "    charge_amount: Optional[float] = Field(default=None, description=\"Disputed charge amount\")\n",
    "    billing_issue: str = Field(description=\"Description of the billing problem\")\n",
    "    resolution: str = Field(description=\"Requested resolution: refund, credit, or investigation\")\n",
    "    urgency: str = Field(description=\"low, medium, or high\")\n",
    "\n",
    "\n",
    "class TechnicalTicket(BaseModel):\n",
    "    \"\"\"Structured output for technical issues.\"\"\"\n",
    "    customer_name: str = Field(description=\"Customer's full name\")\n",
    "    email: str = Field(description=\"Customer's email\")\n",
    "    product_version: Optional[str] = Field(default=None, description=\"Software/product version\")\n",
    "    error_description: str = Field(description=\"What's going wrong\")\n",
    "    steps_to_reproduce: Optional[str] = Field(default=None, description=\"How to reproduce the issue\")\n",
    "    urgency: str = Field(description=\"low, medium, or high\")\n",
    "\n",
    "\n",
    "class AccountTicket(BaseModel):\n",
    "    \"\"\"Structured output for account issues.\"\"\"\n",
    "    customer_name: str = Field(description=\"Customer's full name\")\n",
    "    email: str = Field(description=\"Customer's email\")\n",
    "    account_issue: str = Field(description=\"Description of the account problem\")\n",
    "    action_requested: str = Field(description=\"What account action is needed\")\n",
    "    urgency: str = Field(description=\"low, medium, or high\")\n",
    "\n",
    "\n",
    "print(\"Models defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specialist agents defined.\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────\n",
    "# 2. Define specialist agents (like Rasa forms)\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "billing_agent = Agent(\n",
    "    name=\"Billing Specialist\",\n",
    "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n",
    "    You are a billing support specialist. Have a natural, empathetic conversation\n",
    "    to understand the customer's billing issue.\n",
    "    \n",
    "    Collect through natural conversation:\n",
    "    - Customer name and email\n",
    "    - The charge amount in question (if applicable)\n",
    "    - Description of the billing issue\n",
    "    - What resolution they want (refund, credit, or investigation)\n",
    "    - How urgent this is\n",
    "    \n",
    "    Ask follow-up questions naturally — don't interrogate. Once you have all info,\n",
    "    confirm with the customer and then produce your structured output.\"\"\",\n",
    "    handoff_description=\"Handles billing inquiries, refunds, payment issues, and invoice questions\",\n",
    "    output_type=BillingTicket,\n",
    ")\n",
    "\n",
    "technical_agent = Agent(\n",
    "    name=\"Technical Support\",\n",
    "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n",
    "    You are a technical support specialist. Have a natural conversation to \n",
    "    diagnose and document the customer's technical issue.\n",
    "    \n",
    "    Collect through natural conversation:\n",
    "    - Customer name and email\n",
    "    - Product/software version\n",
    "    - What's going wrong (error messages, unexpected behavior)\n",
    "    - Steps to reproduce (if they can describe them)\n",
    "    - Urgency level\n",
    "    \n",
    "    Be patient and ask clarifying questions. Once you have enough info,\n",
    "    confirm and produce your structured output.\"\"\",\n",
    "    handoff_description=\"Handles software bugs, crashes, technical errors, and product issues\",\n",
    "    output_type=TechnicalTicket,\n",
    ")\n",
    "\n",
    "account_agent = Agent(\n",
    "    name=\"Account Specialist\",\n",
    "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n",
    "    You are an account management specialist. Have a natural conversation\n",
    "    about the customer's account issue.\n",
    "    \n",
    "    Collect through natural conversation:\n",
    "    - Customer name and email\n",
    "    - Description of the account problem (locked out, need to change plan, etc.)\n",
    "    - What action they need taken\n",
    "    - Urgency level\n",
    "    \n",
    "    Be helpful and reassuring. Once you have all info, confirm and produce\n",
    "    your structured output.\"\"\",\n",
    "    handoff_description=\"Handles account access, plan changes, cancellations, and profile updates\",\n",
    "    output_type=AccountTicket,\n",
    ")\n",
    "\n",
    "print(\"Specialist agents defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triage agent defined with handoffs to: ['Billing Specialist', 'Technical Support', 'Account Specialist']\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────\n",
    "# 3. Define the triage agent (like Rasa's intent router / stories)\n",
    "# ──────────────────────────────────────────────\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Customer Support Triage\",\n",
    "    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n",
    "    You are the first point of contact for customer support.\n",
    "    \n",
    "    Greet the customer warmly and determine the nature of their request.\n",
    "    Based on the conversation, hand off to the appropriate specialist:\n",
    "    \n",
    "    - Billing issues (charges, refunds, invoices, payments) → Billing Specialist\n",
    "    - Technical issues (bugs, crashes, errors, product problems) → Technical Support\n",
    "    - Account issues (access, plan changes, cancellations, profile) → Account Specialist\n",
    "    \n",
    "    Ask a clarifying question or two if needed, then hand off. Don't try to solve\n",
    "    the problem yourself — route to the right specialist.\"\"\",\n",
    "    handoffs=[billing_agent, technical_agent, account_agent],\n",
    ")\n",
    "\n",
    "print(\"Triage agent defined with handoffs to:\", [a.name for a in [billing_agent, technical_agent, account_agent]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Single-shot run (agent handles entire conversation autonomously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final agent: Billing Specialist\n",
      "Output type: <class '__main__.BillingTicket'>\n",
      "\n",
      "{\n",
      "  \"customer_name\": \"Sarah Chen\",\n",
      "  \"email\": \"sarah@example.com\",\n",
      "  \"charge_amount\": 49.99,\n",
      "  \"billing_issue\": \"Customer was double-charged $49.99 on her last bill.\",\n",
      "  \"resolution\": \"Refund requested as soon as possible.\",\n",
      "  \"urgency\": \"high\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Single-shot: customer describes everything in one message\n",
    "# The triage agent routes to the right specialist, who produces structured output\n",
    "\n",
    "result = await Runner.run(\n",
    "    triage_agent,\n",
    "    \"Hi, I'm Sarah Chen (sarah@example.com). I've been double-charged $49.99 on my last bill. \"\n",
    "    \"This is really urgent — I need a refund as soon as possible.\",\n",
    ")\n",
    "\n",
    "print(f\"Final agent: {result.last_agent.name}\")\n",
    "print(f\"Output type: {type(result.final_output)}\")\n",
    "print()\n",
    "\n",
    "# The output is a structured Pydantic object from whichever specialist handled it\n",
    "if isinstance(result.final_output, str):\n",
    "    # Triage agent responded directly (shouldn't happen with good prompting)\n",
    "    print(\"Triage response:\", result.final_output)\n",
    "else:\n",
    "    # Specialist produced structured output\n",
    "    print(result.final_output.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Multi-turn conversation with session memory (most Rasa-like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import SQLiteSession\n",
    "\n",
    "# SQLiteSession automatically persists conversation across turns\n",
    "# (like Rasa's tracker store)\n",
    "\n",
    "session = SQLiteSession(\"customer_session_001\", \"conversations.db\")\n",
    "\n",
    "\n",
    "async def multi_turn_support():\n",
    "    \"\"\"Simulate a multi-turn conversation where info is gathered over time.\"\"\"\n",
    "\n",
    "    # Turn 1: Customer initiates vaguely\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TURN 1\")\n",
    "    result = await Runner.run(\n",
    "        triage_agent,\n",
    "        \"Hey, I need some help with something on my account.\",\n",
    "        session=session,\n",
    "    )\n",
    "    print(f\"[{result.last_agent.name}]: {result.final_output}\")\n",
    "    print()\n",
    "\n",
    "    # Turn 2: Customer clarifies it's a technical issue\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TURN 2\")\n",
    "    result = await Runner.run(\n",
    "        triage_agent,\n",
    "        \"The app keeps crashing when I try to generate reports. I'm on version 4.1.\",\n",
    "        session=session,\n",
    "    )\n",
    "    print(f\"[{result.last_agent.name}]: {result.final_output}\")\n",
    "    print()\n",
    "\n",
    "    # Turn 3: Customer provides remaining details\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TURN 3\")\n",
    "    result = await Runner.run(\n",
    "        triage_agent,\n",
    "        \"I'm Mike Johnson, mike.j@company.com. It happens every time I click Export > PDF. \"\n",
    "        \"Started after yesterday's update. This is high priority — my team can't do their weekly reports.\",\n",
    "        session=session,\n",
    "    )\n",
    "    print(f\"[{result.last_agent.name}]:\")\n",
    "    print(f\"Output type: {type(result.final_output)}\")\n",
    "\n",
    "    if not isinstance(result.final_output, str):\n",
    "        print(result.final_output.model_dump_json(indent=2))\n",
    "    else:\n",
    "        print(result.final_output)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "multi_turn_result = await multi_turn_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Interactive multi-turn with real user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def interactive_agent_loop():\n",
    "    \"\"\"\n",
    "    Interactive loop using the Agents SDK — type messages, the triage agent\n",
    "    routes to specialists, and you get a Pydantic object at the end.\n",
    "    Type 'quit' to exit.\n",
    "    \"\"\"\n",
    "    session = SQLiteSession(\"interactive_session\", \"conversations.db\")\n",
    "    current_agent = triage_agent\n",
    "\n",
    "    print(\"Customer Support System (type 'quit' to exit)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in (\"quit\", \"exit\"):\n",
    "            print(\"Session ended.\")\n",
    "            return None\n",
    "\n",
    "        result = await Runner.run(\n",
    "            current_agent,\n",
    "            user_input,\n",
    "            session=session,\n",
    "        )\n",
    "\n",
    "        # Track which agent is handling (triage may hand off to specialist)\n",
    "        current_agent = result.last_agent\n",
    "\n",
    "        # Check if we got structured output (specialist finished)\n",
    "        if not isinstance(result.final_output, str):\n",
    "            print(f\"\\n[{result.last_agent.name}] produced structured output:\")\n",
    "            print(result.final_output.model_dump_json(indent=2))\n",
    "            return result.final_output\n",
    "        else:\n",
    "            print(f\"\\n[{result.last_agent.name}]: {result.final_output}\")\n",
    "\n",
    "\n",
    "# Run it\n",
    "final_output = await interactive_agent_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the final structured output\n",
    "if final_output:\n",
    "    print(f\"Output type: {type(final_output).__name__}\")\n",
    "    print(final_output.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Rasa vs OpenAI Agents SDK — Comparison\n",
    "\n",
    "| Feature | Rasa | OpenAI Agents SDK |\n",
    "|---|---|---|\n",
    "| **Intent Classification** | NLU pipeline (DIET, etc.) | LLM does it natively |\n",
    "| **Routing / Stories** | Stories + Rules YAML | Triage Agent + `handoffs=[]` |\n",
    "| **Form Filling** | Forms + Slots (YAML) | Agent `instructions` + `output_type` |\n",
    "| **Slot Types** | Predefined (text, bool, float, etc.) | Any Pydantic model |\n",
    "| **Conversation Memory** | Tracker Store (Redis, SQL, etc.) | `SQLiteSession` (or custom) |\n",
    "| **Validation** | Slot validation methods | Pydantic validators + Guardrails |\n",
    "| **Custom Actions** | Action server (Python) | Agent tools (Python functions) |\n",
    "| **Training Required** | Yes (NLU + stories data) | No — prompt-based |\n",
    "| **Multi-agent** | Limited | Native handoffs between agents |\n",
    "\n",
    "### Key Takeaway\n",
    "The Agents SDK replaces most of Rasa's YAML configuration with Python code and LLM prompts. You get more flexibility and no training data requirements, at the cost of LLM API calls per turn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
